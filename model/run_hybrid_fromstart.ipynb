{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import u_net\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "X_TRAIN_PATH = '../datasets/Dataset_UHCSDB/Patched/images_patched/'\n",
    "Y_TRAIN_PATH = '../datasets/Dataset_UHCSDB/Patched/labels_png_patched/'\n",
    "METADATA_PATH = '../datasets/Dataset_UHCSDB/UHCSDB_Metadata_Segmentierung.xlsx'\n",
    "\n",
    "def load_and_augment_data(image_path, mask_path, augment_data=True):\n",
    "    images = []\n",
    "    masks = []\n",
    "    angles = [90, 180, 270]\n",
    "    \n",
    "    # Load metadata from the Excel file\n",
    "    metadata = pd.read_excel(METADATA_PATH)\n",
    "\n",
    "    for file in os.listdir(image_path):\n",
    "        if file.endswith('.png'):\n",
    "            img = cv2.imread(os.path.join(image_path, file), cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Extract the corresponding metadata row for the current image\n",
    "            meta_row = metadata[metadata['patch_id'] == file]\n",
    "            \n",
    "            # Ensure metadata exists for the current image\n",
    "            if not meta_row.empty:\n",
    "                scale = meta_row['scale_um_per_px'].values[0] # float between 0.0 and 1.0\n",
    "                microconstituent = meta_row['primary_microconstituent'].values[0] # 'spheroidite' or 'spheroidite+widmanstatten'\n",
    "                # convert microconstituent to a numerical label 0 or 1\n",
    "                if microconstituent == 'spheroidite':\n",
    "                    microconstituent = 0\n",
    "                else:\n",
    "                    microconstituent = 1\n",
    "                anneal_time_min = meta_row['anneal_time_min'].values[0] # int, could be '-' though\n",
    "                if anneal_time_min == '-':\n",
    "                    continue\n",
    "                    anneal_time_min = 0\n",
    "                    anneal_temp_c = 0\n",
    "                    cooling_type = 0\n",
    "                else:\n",
    "                    anneal_temp_c = meta_row['anneal_temperature_C'].values[0] # int\n",
    "                    cooling_type = meta_row['cooling'].values[0] # 'AR' or 'Q'\n",
    "                    # convert cooling type to a numerical label 0 or 1\n",
    "                    if cooling_type == 'AR':\n",
    "                        cooling_type = 0\n",
    "                    else:\n",
    "                        cooling_type = 1\n",
    "                \n",
    "                # Add metadata to image in the form of 5 new channels\n",
    "                meta_channels = np.zeros((img.shape[0], img.shape[1], 5), dtype=np.float32)\n",
    "                meta_channels[..., 0] = scale\n",
    "                meta_channels[..., 1] = microconstituent\n",
    "                meta_channels[..., 2] = anneal_time_min / 1440.0\n",
    "                meta_channels[..., 3] = anneal_temp_c / 1000.0\n",
    "                meta_channels[..., 4] = cooling_type\n",
    "\n",
    "                img = np.concatenate([img[..., np.newaxis], meta_channels.astype(np.float32)], axis=-1)\n",
    "\n",
    "            mask = cv2.imread(os.path.join(mask_path, file), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            if file == 'uhcs0312_patch2.png':\n",
    "                global visualization_image\n",
    "                visualization_image = img\n",
    "                global visualization_mask\n",
    "                visualization_mask = mask\n",
    "            else:\n",
    "                images.append(img)\n",
    "                masks.append(mask)\n",
    "\n",
    "    augmented_images = []\n",
    "    augmented_masks = []\n",
    "\n",
    "    for img, mask in tqdm(zip(images, masks), total=len(images), desc=\"Augmenting data\"):\n",
    "        augmented_images.append(img)\n",
    "        augmented_masks.append(mask)\n",
    "        if augment_data:\n",
    "            for angle in angles:\n",
    "                rotated_img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE if angle == 90 else cv2.ROTATE_180 if angle == 180 else cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "                rotated_mask = cv2.rotate(mask, cv2.ROTATE_90_CLOCKWISE if angle == 90 else cv2.ROTATE_180 if angle == 180 else cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "                augmented_images.append(rotated_img)\n",
    "                augmented_masks.append(rotated_mask)\n",
    "\n",
    "    return augmented_images, augmented_masks\n",
    "\n",
    "n_classes = 4\n",
    "train_images, train_masks = load_and_augment_data(X_TRAIN_PATH, Y_TRAIN_PATH, True)\n",
    "\n",
    "print(\"Number of training images:\", len(train_images))\n",
    "print(\"Shape of a single training image:\", train_images[0].shape)\n",
    "print(\"Number of training masks:\", len(train_masks))\n",
    "print(\"Shape of a single training mask:\", train_masks[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b9cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_images, train_masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Turn the lists into numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print(\"Class values in Y_train:\")\n",
    "unique_classes = np.unique(Y_train)\n",
    "print(unique_classes)   # [0 1 2 3]\n",
    "\n",
    "# print shape of X_train and Y_train\n",
    "print(\"Shape of X_train:\", X_train.shape)  # (number_of_images, height, width, channels)\n",
    "print(\"Shape of Y_train:\", Y_train.shape)  # (number_of_images, height, width, channels)\n",
    "# both are (76, 322, 322)\n",
    "\n",
    "# Convert Y_train to categorical\n",
    "from keras.utils import to_categorical\n",
    "train_masks_cat = to_categorical(Y_train, num_classes=n_classes)\n",
    "Y_train_cat = train_masks_cat.reshape((Y_train.shape[0], Y_train.shape[1], Y_train.shape[2], n_classes))\n",
    "\n",
    "test_masks_cat = to_categorical(Y_test, num_classes=n_classes)\n",
    "Y_test_cat = test_masks_cat.reshape((Y_test.shape[0], Y_test.shape[1], Y_test.shape[2], n_classes))\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(Y_train),\n",
    "    y=Y_train.flatten()\n",
    ")\n",
    "# Normalize class weights\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "print(\"Class weights:\", class_weights)  # [0.338 1.862 2.514 9.083]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82519f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SM_FRAMEWORK'] = 'tf.keras'\n",
    "import segmentation_models as sm\n",
    "\n",
    "def selfmade_model():\n",
    "    import u_net\n",
    "    model = u_net.unet_model_same_padding(in_channels=6)  # 5 metadata channels + 1 grayscale channel\n",
    "    return model\n",
    "\n",
    "def pretrained_model(X_train, X_test, n_classes):\n",
    "    BACKBONE = 'resnet34'\n",
    "    preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "    X_train = preprocess_input(X_train)\n",
    "    X_test = preprocess_input(X_test)\n",
    "    model = sm.Unet(backbone_name=BACKBONE, encoder_weights='imagenet', activation='softmax', classes=n_classes)\n",
    "    return model\n",
    "\n",
    "def compile_with_hybrid_loss(model):\n",
    "    # Custom loss function combining Dice and Focal Loss and 2 different metrics\n",
    "    LR = 0.0001\n",
    "    optimizer = tf.keras.optimizers.Adam(LR)\n",
    "    dice_loss = sm.losses.DiceLoss(class_weights=class_weights)\n",
    "    focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "    total_loss = dice_loss + focal_loss\n",
    "    metrics = [\n",
    "        sm.metrics.IOUScore(threshold=0.5, class_weights=class_weights),\n",
    "        sm.metrics.FScore(threshold=0.5, class_weights=class_weights)\n",
    "    ]\n",
    "    model.compile(optimizer=optimizer, loss=total_loss, metrics=metrics)\n",
    "\n",
    "def compile_with_jaccard_loss(model):\n",
    "    # This is what they used in the segmentation_models library tutorial\n",
    "    # (https://github.com/qubvel/segmentation_models/blob/master/docs/tutorial.rst)\n",
    "    model.compile('Adam', loss=sm.losses.bce_jaccard_loss, metrics=[sm.metrics.iou_score])\n",
    "\n",
    "\n",
    "#model = pretrained_model(X_train, X_test, n_classes)\n",
    "model = selfmade_model()\n",
    "#compile_with_jaccard_loss(model)\n",
    "compile_with_hybrid_loss(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e4f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train_cat,\n",
    "    batch_size=4,\n",
    "    verbose=1,\n",
    "    epochs=50,\n",
    "    validation_data=(X_test, Y_test_cat),\n",
    "    shuffle=True,\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        model_checkpoint\n",
    "        ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('best models/best_small_cross_entropy.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcb5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('best models/best_large_focal_loss.keras', custom_objects={\n",
    "    'jaccard_coeff_multiclass': u_net.jaccard_coeff_multiclass\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SM_FRAMEWORK'] = 'tf.keras'\n",
    "import segmentation_models as sm\n",
    "\n",
    "#model = tf.keras.models.load_model('best models/best_large_mixed_loss.keras', compile=False)\n",
    "model = tf.keras.models.load_model('best_model.keras', compile=False)\n",
    "\n",
    "LR = 0.0001\n",
    "optimizer = tf.keras.optimizers.Adam(LR)\n",
    "dice_loss = sm.losses.DiceLoss(class_weights=class_weights)\n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "total_loss = dice_loss + focal_loss\n",
    "metrics = [\n",
    "    sm.metrics.IOUScore(threshold=0.5, class_weights=class_weights),\n",
    "    sm.metrics.FScore(threshold=0.5, class_weights=class_weights)\n",
    "]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=total_loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3722086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = model.evaluate(X_test, Y_test_cat, batch_size=4, verbose=1)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test Loss: {results[0]}\")  # Loss is the first value\n",
    "for i, metric in enumerate(metrics, start=1):  # Metrics start from the second value\n",
    "\tprint(f\"Test {metric.name}: {results[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IOU\n",
    "y_pred = model.predict(X_test, batch_size=4)\n",
    "y_pred_argmax=np.argmax(y_pred, axis=3)\n",
    "\n",
    "#Using built in keras function\n",
    "from keras.metrics import MeanIoU\n",
    "n_classes = 4\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(Y_test, y_pred_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "\n",
    "#To calculate I0U for each class...\n",
    "values = IOU_keras.total_cm.numpy()\n",
    "print(values)\n",
    "class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3] + values[1,0]+ values[2,0]+ values[3,0])\n",
    "class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3] + values[0,1]+ values[2,1]+ values[3,1])\n",
    "class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3] + values[0,2]+ values[1,2]+ values[3,2])\n",
    "class4_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2] + values[0,3]+ values[1,3]+ values[2,3])\n",
    "\n",
    "print(\"IoU for class1 is: \", class1_IoU)\n",
    "print(\"IoU for class2 is: \", class2_IoU)\n",
    "print(\"IoU for class3 is: \", class3_IoU)\n",
    "print(\"IoU for class4 is: \", class4_IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bbf013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the training history\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['iou_score'])\n",
    "    plt.plot(history.history['val_iou_score'])\n",
    "    plt.title('Model IoU')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.seed(43)\n",
    "# Pick a random test image\n",
    "import matplotlib.pyplot as plt\n",
    "idx = random.randint(0, X_test.shape[0] - 1)\n",
    "test_img = X_test[idx]\n",
    "true_mask = Y_test[idx]\n",
    "\n",
    "test_img = visualization_image\n",
    "true_mask = visualization_mask\n",
    "# Rotate the test image and true mask twice\n",
    "test_img = cv2.rotate(test_img, cv2.ROTATE_90_CLOCKWISE)\n",
    "test_img = cv2.rotate(test_img, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "true_mask = cv2.rotate(true_mask, cv2.ROTATE_90_CLOCKWISE)\n",
    "true_mask = cv2.rotate(true_mask, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "# Predict the mask\n",
    "pred_mask = model.predict(np.expand_dims(test_img, axis=0))\n",
    "pred_mask = np.argmax(pred_mask, axis=-1)[0]\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Test Image\")\n",
    "plt.imshow(test_img[..., 0], cmap='gray')  # Display the first channel\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"True Mask\")\n",
    "plt.imshow(true_mask, cmap='nipy_spectral', vmin=0, vmax=n_classes-1)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Predicted Mask\")\n",
    "plt.imshow(pred_mask, cmap='nipy_spectral', vmin=0, vmax=n_classes-1)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001adce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

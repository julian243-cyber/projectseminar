{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "X_TRAIN_PATH = '../datasets/Dataset_UHCSDB/Patched/images_patched/'\n",
    "Y_TRAIN_PATH = '../datasets/Dataset_UHCSDB/Patched/labels_png_patched/'\n",
    "\n",
    "X_TRAIN_PATH_NEW = '../datasets/Dataset_KG/images/'\n",
    "Y_TRAIN_PATH_NEW = '../datasets/Dataset_KG/masks/'\n",
    "\n",
    "# Some images have an obstructing label on them so we ignore those\n",
    "IGNORE_IMAGES = [\n",
    "    '10Cs 11_patch_8.png', 'U-510 coeur_Champ_1_1_5_patch_24.png', 'V-294 peau_Champ_1_1_1_patch_24.png', 'v-448 b mr_Champ_1_1_4_patch_24.png', 'W-441_Champ_1_1_4_patch_24.png',\n",
    "]\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "def load_and_augment_data(image_path, mask_path):\n",
    "    images = []\n",
    "    masks = []\n",
    "    angles = [90, 180, 270]\n",
    "\n",
    "    for file in os.listdir(image_path):\n",
    "        if file.endswith('.png') and file not in IGNORE_IMAGES:\n",
    "            img = cv2.imread(os.path.join(image_path, file), cv2.IMREAD_GRAYSCALE)\n",
    "            images.append(img)\n",
    "            mask = cv2.imread(os.path.join(mask_path, file), cv2.IMREAD_GRAYSCALE)\n",
    "            masks.append(mask)\n",
    "\n",
    "    augmented_images = []\n",
    "    augmented_masks = []\n",
    "\n",
    "    for img, mask in tqdm(zip(images, masks), total=len(images), desc=\"Augmenting data\"):\n",
    "        augmented_images.append(img)\n",
    "        augmented_masks.append(mask)\n",
    "        #for angle in angles:\n",
    "        #    rotated_img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE if angle == 90 else cv2.ROTATE_180 if angle == 180 else cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        #    rotated_mask = cv2.rotate(mask, cv2.ROTATE_90_CLOCKWISE if angle == 90 else cv2.ROTATE_180 if angle == 180 else cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        #    augmented_images.append(rotated_img)\n",
    "        #    augmented_masks.append(rotated_mask)\n",
    "\n",
    "    return augmented_images, augmented_masks\n",
    "\n",
    "#train_images, train_masks = load_and_augment_data(X_TRAIN_PATH, Y_TRAIN_PATH)\n",
    "train_images, train_masks = load_and_augment_data(X_TRAIN_PATH_NEW, Y_TRAIN_PATH_NEW)\n",
    "\n",
    "print(\"Number of training images:\", len(train_images))\n",
    "print(\"Shape of a single training image:\", train_images[0].shape)\n",
    "print(\"Number of training masks:\", len(train_masks))\n",
    "print(\"Shape of a single training mask:\", train_masks[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b9cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_images, train_masks, test_size=0.2, random_state=42)\n",
    "\n",
    "# Turn the lists into numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "print(\"Class values in Y_train:\")\n",
    "unique_classes = np.unique(Y_train)\n",
    "print(unique_classes)   # [0 1 2 3]\n",
    "\n",
    "# print shape of X_train and Y_train\n",
    "print(\"Shape of X_train:\", X_train.shape)  # (number_of_images, height, width, channels)\n",
    "print(\"Shape of Y_train:\", Y_train.shape)  # (number_of_images, height, width, channels)\n",
    "# both are (76, 322, 322)\n",
    "\n",
    "# Convert Y_train to categorical\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# for 2 classes\n",
    "Y_train = (Y_train > 0).astype(np.uint8)\n",
    "Y_test = (Y_test > 0).astype(np.uint8)\n",
    "\n",
    "train_masks_cat = to_categorical(Y_train, num_classes=n_classes)\n",
    "Y_train_cat = train_masks_cat.reshape((Y_train.shape[0], Y_train.shape[1], Y_train.shape[2], n_classes))\n",
    "\n",
    "test_masks_cat = to_categorical(Y_test, num_classes=n_classes)\n",
    "Y_test_cat = test_masks_cat.reshape((Y_test.shape[0], Y_test.shape[1], Y_test.shape[2], n_classes))\n",
    "\n",
    "#from sklearn.utils import class_weight\n",
    "#class_weights = class_weight.compute_class_weight(\n",
    "#    class_weight='balanced',\n",
    "#    classes=np.unique(Y_train),\n",
    "#    y=Y_train.flatten()\n",
    "#)\n",
    "# Normalize class weights\n",
    "#class_weights = class_weights / class_weights.sum()\n",
    "#print(\"Class weights:\", class_weights)  # [0.338 1.862 2.514 9.083]\n",
    "class_weights = [0.82500247, 0.17499753]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e4f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import u_net, u_net_512\n",
    "model = u_net_512.unet_model_same_padding()\n",
    "#model.compile(optimizer='adam', loss=[u_net.make_weighted_jaccard_loss(class_weights=class_weights)], metrics=[u_net.jaccard_coeff_multiclass])\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[u_net.jaccard_coeff_multiclass])\n",
    "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer='adam', loss='categorical_focal_crossentropy', metrics=[u_net.jaccard_coeff_multiclass])\n",
    "\n",
    "os.environ['SM_FRAMEWORK'] = 'tf.keras'\n",
    "import segmentation_models as sm\n",
    "\n",
    "LR = 0.0001\n",
    "optimizer = tf.keras.optimizers.Adam(LR)\n",
    "dice_loss = sm.losses.DiceLoss(class_weights=class_weights)\n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "total_loss = dice_loss + focal_loss\n",
    "metrics = [\n",
    "    sm.metrics.IOUScore(threshold=0.5, class_weights=class_weights),\n",
    "    sm.metrics.FScore(threshold=0.5, class_weights=class_weights)\n",
    "]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=total_loss, metrics=metrics)\n",
    "\n",
    "# Callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train_cat,\n",
    "    batch_size=4,\n",
    "    verbose=1,\n",
    "    epochs=50,\n",
    "    validation_data=(X_test, Y_test_cat),\n",
    "    shuffle=True,\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        model_checkpoint\n",
    "        ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('best models/best_small_cross_entropy.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcb5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('best models/best_large_focal_loss.keras', custom_objects={\n",
    "    'jaccard_coeff_multiclass': u_net.jaccard_coeff_multiclass\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SM_FRAMEWORK'] = 'tf.keras'\n",
    "import segmentation_models as sm\n",
    "\n",
    "model = tf.keras.models.load_model('best models/best_large_mixed_loss.keras', compile=False)\n",
    "\n",
    "LR = 0.0001\n",
    "optimizer = tf.keras.optimizers.Adam(LR)\n",
    "dice_loss = sm.losses.DiceLoss(class_weights=class_weights)\n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "total_loss = dice_loss + focal_loss\n",
    "metrics = [\n",
    "    sm.metrics.IOUScore(threshold=0.5, class_weights=class_weights),\n",
    "    sm.metrics.FScore(threshold=0.5, class_weights=class_weights)\n",
    "]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=total_loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3722086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# Evaluate the model in smaller batches to avoid running out of memory\n",
    "loss, accuracy = model.evaluate(X_test, Y_test_cat, batch_size=4, verbose=1)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IOU FOR OLD DATASET\n",
    "y_pred = model.predict(X_test, batch_size=4)\n",
    "y_pred_argmax=np.argmax(y_pred, axis=3)\n",
    "\n",
    "#Using built in keras function\n",
    "from keras.metrics import MeanIoU\n",
    "n_classes = 4\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(Y_test, y_pred_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "\n",
    "#To calculate I0U for each class...\n",
    "values = IOU_keras.total_cm.numpy()\n",
    "print(values)\n",
    "class1_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3] + values[1,0]+ values[2,0]+ values[3,0])\n",
    "class2_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3] + values[0,1]+ values[2,1]+ values[3,1])\n",
    "class3_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3] + values[0,2]+ values[1,2]+ values[3,2])\n",
    "class4_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2] + values[0,3]+ values[1,3]+ values[2,3])\n",
    "\n",
    "print(\"IoU for class1 is: \", class1_IoU)\n",
    "print(\"IoU for class2 is: \", class2_IoU)\n",
    "print(\"IoU for class3 is: \", class3_IoU)\n",
    "print(\"IoU for class4 is: \", class4_IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import MeanIoU\n",
    "\n",
    "# IOU FOR NEW DATASET\n",
    "y_pred_new = model.predict(X_test, batch_size=4)\n",
    "y_pred_argmax_new = np.argmax(y_pred_new, axis=3)\n",
    "\n",
    "# Using built-in Keras function to calculate Mean IoU\n",
    "IOU_keras_new = MeanIoU(num_classes=n_classes)\n",
    "IOU_keras_new.update_state(Y_test, y_pred_argmax_new)\n",
    "print(\"Mean IoU for new dataset =\", IOU_keras_new.result().numpy())\n",
    "\n",
    "# To calculate IoU for each class\n",
    "values_new = IOU_keras_new.total_cm.numpy()\n",
    "print(values_new)\n",
    "class1_IoU_new = values_new[0, 0] / (values_new[0, 0] + values_new[0, 1] + values_new[1, 0])\n",
    "class2_IoU_new = values_new[1, 1] / (values_new[1, 1] + values_new[1, 0] + values_new[0, 1])\n",
    "\n",
    "print(\"IoU for class1 (new dataset):\", class1_IoU_new)\n",
    "print(\"IoU for class2 (new dataset):\", class2_IoU_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bbf013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the training history\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['iou_score'])\n",
    "    plt.plot(history.history['val_iou_score'])\n",
    "    plt.title('Model IoU')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.seed(42)\n",
    "# Pick a random test image\n",
    "import matplotlib.pyplot as plt\n",
    "idx = random.randint(0, X_test.shape[0] - 1)\n",
    "test_img = X_test[idx]\n",
    "true_mask = Y_test[idx]\n",
    "\n",
    "# Predict the mask\n",
    "pred_mask = model.predict(np.expand_dims(test_img, axis=0))\n",
    "pred_mask = np.argmax(pred_mask, axis=-1)[0]\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Test Image\")\n",
    "plt.imshow(test_img, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"True Mask\")\n",
    "plt.imshow(true_mask, cmap='nipy_spectral', vmin=0, vmax=n_classes-1)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Predicted Mask\")\n",
    "plt.imshow(pred_mask, cmap='nipy_spectral', vmin=0, vmax=n_classes-1)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001adce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
